{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Doc2Vec Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "#function reads data file\n",
    "def read_text_file(f):\n",
    "    df_complete = pd.read_csv(f)\n",
    "    df = df_complete.loc[:,[\"sentiment\",\"comment\"]]  #add label to comments\n",
    "    df.dropna(how=\"any\", inplace=True) #drops columns that are not used\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sentiment                                            comment\n",
      "0            1  She present class materials with powerpoint wh...\n",
      "1            1  The instructor was generally quite good at exp...\n",
      "2            0  I cant really tell how effective the instructi...\n",
      "3            1  She did a good job of explaining the logic beh...\n",
      "4            1  The activities we did in classed where explain...\n",
      "5            1              It was very informative and thourough\n",
      "6            0  Sometimes hard to understand and catch up with...\n",
      "7            1  Teaching was good with lecture and few example...\n",
      "8            1  The teaching was excellent as a whole there wa...\n",
      "9            1  Prof Koufakou did a good job teaching the clas...\n",
      "10           1  The teaching seemed well thought out and the i...\n",
      "11           1  Your teaching style is good the mixture of lec...\n",
      "12           1  The information was presented very well with i...\n",
      "13           1  The instructor taught the lecture to accompany...\n",
      "14           1  Everything was explained through the use of Po...\n",
      "15           1  Instructor clearly spoke from personal knowled...\n",
      "16           1  I very much liked how you would explain someth...\n",
      "17           1  The teacher presented information in a clear w...\n",
      "18           1  I think the teaching was very good in my opini...\n",
      "19           0  Teaching was absolutely fine but the class fel...\n",
      "20           1  She did a good job explaining a large some of ...\n",
      "21           1  Class was organized in a way that I could unde...\n",
      "22           1  Overall I think the information was presented ...\n",
      "23           0  Classes were lectures that were a little hard ...\n",
      "24           1  The instructor has well presented and explaine...\n",
      "25           0  Some lessons were difficult to follow I think ...\n",
      "26           0  In my view the instructor failed to notice tha...\n",
      "27           1                                               Nice\n",
      "28           0  Actually present the power points it doesnt he...\n",
      "29           0  very through but sometimes deemed information ...\n",
      "..         ...                                                ...\n",
      "513          0  As whole the teaching was a B+ for me. The mat...\n",
      "514          0                  Generally it was a lecture class.\n",
      "515          1  The instructor's teaching strategy consists of...\n",
      "516          1  The teaching was very well done The lecture co...\n",
      "517          1  excellent. Very informative and explains the t...\n",
      "518          1  The lectures were primarily based off of the P...\n",
      "519          0  Presentations were boring. Probably would be n...\n",
      "520          0  I zoned out a lot. There was a lot of informat...\n",
      "521          0  The teaching was really good and the power poi...\n",
      "522          1  The professor did a good job explaining the ma...\n",
      "523          1  I think he did a good job teaching overall. Th...\n",
      "524          1  The information was presented alongside the ch...\n",
      "525          1  He has done great job! he has used different t...\n",
      "526          1  Through PowerPoints that had great examples an...\n",
      "527          1         Wonderful. Easy to understand and consume.\n",
      "528          1       Did a great job teaching us the information.\n",
      "529          1  Our professor explains very well and if studen...\n",
      "530          1  The teaching as a whole was well structured. T...\n",
      "531          1  Very well structured. I enjoy how the course s...\n",
      "532          1  Excellent instruction. Dr. [B]'s powerpoint sl...\n",
      "533          1          Powerpoint and lecture. Example programs.\n",
      "534          1  Teaching was pretty good. Assignments made me ...\n",
      "535          1  He basically would lecture all class over a po...\n",
      "536          1                         Explains it pretty clearly\n",
      "537          1  Professor [B] presented the information as wel...\n",
      "538          1  I thought teaching was great but the ball was ...\n",
      "539          1  The instructor went through each chapter and m...\n",
      "540          1                          He was a good instructor.\n",
      "541          1         I liked how the information was presented.\n",
      "542          1  The teaching was good with very helpful emphas...\n",
      "\n",
      "[543 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = read_text_file(\"500/cleaned.csv\")\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vilma\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.8 s\n"
     ]
    }
   ],
   "source": [
    "#Train Doc2Vec - considering each comment a document\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re # for regular expressions\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "w = re.compile(\"\\w+\",re.I) #Matches Unicode word characters; this includes most characters that can be part of a word in any language\n",
    "\n",
    "#Doc2vec only receive labeled sentences so the following method creates a label for each comment\n",
    "def label_sentences(df):\n",
    "    labeled_sentences = []\n",
    "    for index, datapoint in df.iterrows():\n",
    "        tokenized_words = re.findall(w,datapoint[\"comment\"].lower())\n",
    "        labeled_sentences.append(LabeledSentence(words=tokenized_words, tags=['SENT_%s' %index]))\n",
    "    return labeled_sentences\n",
    "\n",
    "\n",
    "def train_doc2vec_model(labeled_sentences):\n",
    "    model = Doc2Vec(min_count=1, window=8, size=100, alpha=0.025, min_alpha=0.025)\n",
    "    \n",
    "    #The following line creates a vocabulary table, digesting all the words and filtering out the unique words, and doing some basic counts on them\n",
    "    model.build_vocab(labeled_sentences)\n",
    "    for epoch in range(10):\n",
    "        #trains Doc2Vec on variable learning rate sequentially decreasing.\n",
    "        model.train(labeled_sentences,total_examples=model.corpus_count, epochs=model.iter)\n",
    "        model.alpha -= 0.002 \n",
    "        model.min_alpha = model.alpha\n",
    "    \n",
    "    return model\n",
    "\n",
    "sen = label_sentences(df)\n",
    "%time model = train_doc2vec_model(sen) #calls to train the model, and gives the time it takes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring a VectorÂ¶\n",
    "One important thing to note is that you can now infer a vector for any piece of text without having to re-train the model by passing a list of words to the model.infer_vector function. This vector can then be compared with other vectors via cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01613971, -0.04091503,  0.00876203, -0.01167556, -0.01995205,\n",
       "       -0.00318355,  0.0410048 ,  0.02143336, -0.01548649, -0.0450041 ,\n",
       "        0.00122282,  0.01447786, -0.04755984,  0.00333679, -0.00152568,\n",
       "        0.02429867, -0.01075239,  0.0137948 ,  0.01199499, -0.04730212,\n",
       "       -0.03136966, -0.03100823, -0.04372935, -0.01686253,  0.03164188,\n",
       "        0.01725915, -0.00215421, -0.01616173,  0.01221084, -0.01466749,\n",
       "       -0.02699424,  0.0463641 ,  0.04878873, -0.01369095, -0.01290173,\n",
       "       -0.02132636, -0.03562954, -0.08159862, -0.01800677,  0.05490265,\n",
       "        0.03383458,  0.02048385, -0.01209165, -0.02024756,  0.04544707,\n",
       "        0.05536291, -0.03576288, -0.02892365, -0.03904415,  0.00911469,\n",
       "       -0.00288884,  0.03717517,  0.00210875,  0.01011741,  0.00641064,\n",
       "       -0.00916962,  0.05623824, -0.01612606, -0.00252074, -0.00154954,\n",
       "        0.03942744,  0.00510509, -0.00520389,  0.03616713,  0.00219202,\n",
       "        0.09004503,  0.01241444,  0.01579709, -0.0244659 ,  0.01948135,\n",
       "        0.04722071,  0.00342519, -0.05260285,  0.02277771,  0.00610737,\n",
       "        0.04198175,  0.00177659, -0.00597762,  0.02045029, -0.04050598,\n",
       "        0.04177275,  0.02949414,  0.01775886,  0.03719503, -0.03443503,\n",
       "       -0.00634886, -0.02195001, -0.00461152, -0.01672953,  0.01361678,\n",
       "        0.00471625, -0.00897813,  0.01511789,  0.00842674, -0.03841146,\n",
       "        0.02973882,  0.00492189,  0.01270972,  0.0123347 ,  0.05028689], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(['only', 'you', 'can', 'prevent', 'forrest', 'fires'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.68987322e-02,  -1.34596065e-01,   1.20176189e-02,\n",
       "        -5.41564040e-02,  -1.38594434e-01,   6.60225004e-02,\n",
       "         3.47515084e-02,   4.00989205e-02,  -2.24182624e-02,\n",
       "        -3.63116749e-02,  -5.61324768e-02,   6.80981427e-02,\n",
       "        -8.86763260e-02,   2.62290295e-02,   1.25036672e-01,\n",
       "         7.40186572e-02,   2.35537700e-02,   5.90865910e-02,\n",
       "         3.97029631e-02,  -5.40015623e-02,  -2.97947600e-02,\n",
       "        -5.16041405e-02,  -9.29372013e-02,  -8.47185850e-02,\n",
       "         1.45436883e-01,  -2.78214320e-05,  -3.22634429e-02,\n",
       "        -9.27570462e-02,  -4.28357068e-03,   4.60200273e-02,\n",
       "        -8.40533674e-02,   6.03971891e-02,   1.19356319e-01,\n",
       "        -1.08101949e-01,  -3.44229974e-02,  -5.53229526e-02,\n",
       "        -1.48277357e-02,  -6.87017590e-02,  -7.39986151e-02,\n",
       "         1.01892285e-01,   1.30582452e-01,   1.03756793e-01,\n",
       "        -4.45642024e-02,   2.01259479e-02,   5.93720600e-02,\n",
       "         1.30347580e-01,  -3.20442840e-02,   4.18784702e-03,\n",
       "        -1.10545151e-01,   3.75190303e-02,   4.14130017e-02,\n",
       "         2.43697539e-02,   8.17022771e-02,   4.55680788e-02,\n",
       "         1.21453650e-01,   3.97757161e-03,   1.37892216e-01,\n",
       "        -1.49517721e-02,  -2.16523185e-02,  -4.79145534e-03,\n",
       "         1.85861886e-02,   2.05826163e-02,  -5.21900877e-03,\n",
       "         7.90595934e-02,  -3.02932803e-02,   2.02657685e-01,\n",
       "         5.42236567e-02,  -1.57798696e-02,  -9.56393480e-02,\n",
       "         1.09079495e-01,   1.67674020e-01,   1.40763875e-02,\n",
       "        -1.79980904e-01,   5.32741174e-02,   3.78162488e-02,\n",
       "         1.78568035e-01,   3.43046151e-02,   3.62852365e-02,\n",
       "         4.67148274e-02,  -1.10142723e-01,   3.04535571e-02,\n",
       "         9.17535871e-02,   7.35412389e-02,   9.80283469e-02,\n",
       "        -1.57381911e-02,   2.70631257e-02,  -5.23714311e-02,\n",
       "        -1.28098443e-01,  -2.88862325e-02,   9.55696851e-02,\n",
       "        -7.49114603e-02,  -1.68673202e-01,   1.16867684e-02,\n",
       "         8.16205293e-02,  -1.25584185e-01,   2.00945381e-02,\n",
       "         5.14027849e-02,   9.35553387e-02,   2.94478703e-02,\n",
       "         1.19441763e-01], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This shows the vector for the first comment labeled as SENT_0\n",
    "model.docvecs['SENT_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('teaching', 0.8877487778663635),\n",
       " ('alright', 0.8867093920707703),\n",
       " ('keeping', 0.8856614828109741),\n",
       " ('instructors', 0.8766233921051025),\n",
       " ('pretty', 0.8694695234298706),\n",
       " ('overall', 0.8670663833618164),\n",
       " ('job', 0.8608330488204956),\n",
       " ('jo', 0.8493382334709167),\n",
       " ('great', 0.8398537635803223),\n",
       " ('hat', 0.8341473937034607)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving and Loading Models\n",
    "model.save('./surveyVectors.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                            comment  \\\n",
      "0          1  She present class materials with powerpoint wh...   \n",
      "\n",
      "                                 vectorized_comments  \n",
      "0  [0.0368987, -0.134596, 0.0120176, -0.0541564, ...  \n"
     ]
    }
   ],
   "source": [
    "#The following method stores the vectorized comments in the array comments[]\n",
    "#and stores its label in the array y. This is done so that we can do\n",
    "#the classification using X and y values.\n",
    "def vectorize_comments(df,d2v_model):\n",
    "    y = []\n",
    "    comments = []\n",
    "    for i in range(0,df.shape[0]):\n",
    "        label = 'SENT_%s' %i\n",
    "        comments.append(d2v_model.docvecs[label])\n",
    "    df['vectorized_comments'] = comments\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = vectorize_comments(df,model)\n",
    "print (df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save vectorized datafrane to pckl file\n",
    "df.to_pickle('vectorComments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cross validation\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import pickle\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# classifiers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import fbeta_score\n",
    "import numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    # Calculates the precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fmeasure(y_true, y_pred):\n",
    "    # Calculates the f-measure, the harmonic mean of precision and recall.\n",
    "    f = 2*(precision(y_true, y_pred)*recall(y_true, y_pred))/(precision(y_true, y_pred)+recall(y_true, y_pred))\n",
    "    #return fbeta_score(y_true, y_pred, beta=1)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_dim=100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', recall, precision,fmeasure])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kerasModel = KerasClassifier(build_fn=create_model, verbose=2)\n",
    "\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "init = ['glorot_uniform', 'normal', 'uniform']\n",
    "epochs = [10]\n",
    "batches = [100]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
    "\n",
    "Kgrid = GridSearchCV(estimator=kerasModel, param_grid=param_grid, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert data arrays to lists\n",
    "X=df[\"vectorized_comments\"].T.tolist()\n",
    "y=df[\"sentiment\"].T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Kgrid.fit(numpy.asarray(X), numpy.asarray(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of the mean scores only\n",
    "Kgrid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# examine the best model\n",
    "print(Kgrid.best_score_)\n",
    "print(Kgrid.best_params_)\n",
    "print(Kgrid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y1_pred = Kgrid.best_estimator_.predict(numpy.asarray(X))\n",
    "print(confusion_matrix(y, y1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
